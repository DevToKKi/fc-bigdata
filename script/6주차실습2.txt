1. master 에  접속 

sudo wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.40/mysql-connector-java-5.1.40.jar

sudo mv mysql-connector-java-5.1.40.jar /usr/lib/spark/jars/mysql-connector-java-5.1.40.jar

2. spark 설정

sudo vi /etc/spark/conf/spark-defaults.conf

---
spark.driver.extraClassPath      :/usr/lib/spark/jars/mysql-connector-java-5.1.40.jar


... 

spark.executor.extraClassPath    :/usr/lib/spark/jars/mysql-connector-java-5.1.40.jar


---


from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .getOrCreate()

jdbcHostname = "zbinfo-d-1.d-d.ap-northeast-1.rds.amazonaws.com"
jdbcDatabase = "spark"
username="data"
password="sparkdatapipe"
jdbcPort = 3306
jdbcUrl = "jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}?characterEncoding=UTF-8".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)

sql="(select * from spark.apart_dau limit 10) a"
jdbcDF = spark.read.format("jdbc").option("driver","com.mysql.jdbc.Driver" ).option("url", jdbcUrl).option("dbtable", sql).option("user", username).option("password", password).load()

jdbcDF.show()






http://redshift-maven-repository.s3-website-us-east-1.amazonaws.com/release

https://jitpack.io


com.github.databricks:spark-redshift_2.11:master-SNAPSHOT

com.amazon.redshift:redshift-jdbc42:1.2.1.1001


/home/hadoop/RedshiftJDBC42-1.2.16.1027.jar


https://jitpack.io


https://index.scala-lang.org/databricks/spark-redshift/spark-redshift/3.0.0-preview1?target=_2.11


select * from information_schema.tables;

--------------------------------------------------------
create external schema redshift_user from data catalog 
database ‘spark' 
iam_role 'arn:aws:iam::0311xxxx584:role/dpRedshiftSpectrum' 
region 'ap-northeast-2';
--------------------------------------------------------


rds endpoint : 

sparkdatapipe.cp7ovqbewwso.ap-northeast-2.rds.amazonaws.com
